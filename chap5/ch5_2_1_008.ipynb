{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from common.constants import NUM_PROCESSES, OUTPUT_DIR, TRAIN_CSV_PATH\n",
    "from common.utils import compute_weights\n",
    "from features.match import build_match_features\n",
    "from features.length import build_length_features\n",
    "from features.edit_distance import build_edit_distance_features\n",
    "from features.word_vector import build_farthest_word_distance_features, build_wmd_features\n",
    "from features.decomposition import DecompositionType, VectorizerType, build_decomposition_features\n",
    "from features.magic import build_magic_features\n",
    "from features.graph import build_graph_connected_component_features, build_graph_link_prediction_features, build_graph_node_features\n",
    "from texts.preprocessing import PreprocessingKey, StopwordsKey, EmbeddingKey\n",
    "from experiments.gbm_common import run_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 64,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"seed\": 1,\n",
    "    \"num_threads\": NUM_PROCESSES,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv(TRAIN_CSV_PATH, na_filter=False)\n",
    "trn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat(\n",
    "    [\n",
    "        build_match_features(PreprocessingKey.NLTK_STEMMING, StopwordsKey.NLTK_STEMMED, 1),\n",
    "        build_match_features(PreprocessingKey.NLTK_STEMMING, StopwordsKey.NLTK_STEMMED, 2),\n",
    "        build_match_features(PreprocessingKey.SPACE_TOKENIZATION, StopwordsKey.NLTK, 1),\n",
    "        build_match_features(PreprocessingKey.SPACE_TOKENIZATION, StopwordsKey.NLTK, 2),\n",
    "        \n",
    "        build_length_features(PreprocessingKey.NLTK_STEMMING, StopwordsKey.NLTK_STEMMED, 1),\n",
    "        build_length_features(PreprocessingKey.NLTK_STEMMING, StopwordsKey.NLTK_STEMMED, 2),\n",
    "        build_length_features(PreprocessingKey.SPACE_TOKENIZATION, StopwordsKey.NLTK, 1),\n",
    "        build_length_features(PreprocessingKey.SPACE_TOKENIZATION, StopwordsKey.NLTK, 2),\n",
    "        \n",
    "        build_edit_distance_features(PreprocessingKey.NLTK_STEMMING, StopwordsKey.NONE),\n",
    "        build_edit_distance_features(PreprocessingKey.SPACE_TOKENIZATION, StopwordsKey.NONE),\n",
    "        \n",
    "        build_wmd_features(\n",
    "            PreprocessingKey.NLTK_TOKENIZATION,\n",
    "            StopwordsKey.NLTK,\n",
    "            EmbeddingKey.GLOVE,\n",
    "            normalize=True,\n",
    "        ),\n",
    "        build_wmd_features(\n",
    "            PreprocessingKey.NLTK_TOKENIZATION,\n",
    "            StopwordsKey.NLTK,\n",
    "            EmbeddingKey.GLOVE,\n",
    "            normalize=False,\n",
    "        ),\n",
    "        build_farthest_word_distance_features(\n",
    "            PreprocessingKey.NLTK_TOKENIZATION,\n",
    "            StopwordsKey.NLTK,\n",
    "            EmbeddingKey.GLOVE,\n",
    "            metric=\"cosine\",\n",
    "            normalize=True\n",
    "        ),\n",
    "        build_farthest_word_distance_features(\n",
    "            PreprocessingKey.NLTK_TOKENIZATION,\n",
    "            StopwordsKey.NLTK,\n",
    "            EmbeddingKey.GLOVE,\n",
    "            metric=\"euclidean\",\n",
    "            normalize=True\n",
    "        ),\n",
    "        build_farthest_word_distance_features(\n",
    "            PreprocessingKey.NLTK_TOKENIZATION,\n",
    "            StopwordsKey.NLTK,\n",
    "            EmbeddingKey.GLOVE,\n",
    "            metric=\"euclidean\",\n",
    "            normalize=False\n",
    "        ),\n",
    "        \n",
    "        build_decomposition_features(\n",
    "            PreprocessingKey.NLTK_STEMMING,\n",
    "            StopwordsKey.NLTK_STEMMED,\n",
    "            VectorizerType.COUNT,\n",
    "            DecompositionType.SVD,\n",
    "            n_components=30,\n",
    "            ngram_range=(1, 2)\n",
    "        ),\n",
    "        build_decomposition_features(\n",
    "            PreprocessingKey.NLTK_STEMMING,\n",
    "            StopwordsKey.NLTK_STEMMED,\n",
    "            VectorizerType.TFIDF_NONE,\n",
    "            DecompositionType.SVD,\n",
    "            n_components=30,\n",
    "            ngram_range=(1, 2)\n",
    "        ),\n",
    "        \n",
    "        build_magic_features(),\n",
    "        \n",
    "        build_graph_connected_component_features(),\n",
    "        build_graph_link_prediction_features(),\n",
    "        build_graph_node_features(),\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = OUTPUT_DIR / \"ch5_2_1_008\"\n",
    "save_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = compute_weights(trn_df[\"is_duplicate\"], 0.174)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_kfold(\n",
    "    features=features,\n",
    "    trn_targets=trn_df.is_duplicate,\n",
    "    n_splits=5,\n",
    "    save_dir=save_dir,\n",
    "    model_params=model_params,\n",
    "    weights=weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
